{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2e60847",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Relation Extraction System using Sequential Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b20281-dfd6-46fb-a7b8-c90e4043c1ec",
   "metadata": {
    "tags": []
   },
   "source": [
    "This notebook deals with handling the text data by deriving relationships between entities in each text input. Instead of just using available relation extraction methods, I tried to incorporate transfer learning model that learnt from a large pool of unstructured dataset and thus further fine-tuning it to labeled data of desired task. <br>\n",
    "This makes the accuracy and the performance go up comparatively. This work is based on a paper named \"Neural Sequential Transfer Learning for Relation Extraction\"  by Christoph Benedikt Alt M.Sc.  ORCID: 0000-0002-0500-8250. knowledge credits to the makers of the paper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e42bb7",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "197d619a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import os\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47336b2e",
   "metadata": {},
   "source": [
    "###  Load a SpaCy Model for Entity Recognition & Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "041547b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a3517e",
   "metadata": {},
   "source": [
    "### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dbcc5158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read and preprocess text files\n",
    "def load_and_preprocess_files(folder_path):\n",
    "    texts = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.txt'):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            with open(file_path, 'r') as file:\n",
    "                content = file.read()\n",
    "                # Preprocess content (example: tokenize)\n",
    "                doc = nlp(content)\n",
    "                tokens = ' '.join([token.text for token in doc])\n",
    "                texts.append(tokens)\n",
    "                \n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "996b6a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = 'C:/Personal/ML/Relation Extraction Project/data/textfiles'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4daee0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = load_and_preprocess_files(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f29b9b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame({'text': texts})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "413c5bb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\n This paper introduces a   system for catego...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\n This paper presents a new approach to   sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\n  This paper describes a domain independent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\n  In this paper , we describe the   pronomi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\n In our current research into the design of ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  \\n This paper introduces a   system for catego...\n",
       "1  \\n This paper presents a new approach to   sta...\n",
       "2   \\n  This paper describes a domain independent...\n",
       "3   \\n  In this paper , we describe the   pronomi...\n",
       "4  \\n In our current research into the design of ..."
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dfd7230",
   "metadata": {},
   "source": [
    "### Initialize a BERT Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6c658afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d7a60a",
   "metadata": {},
   "source": [
    "### Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2ae72383",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    # Process the sentence with spaCy\n",
    "    doc = nlp(sentence)\n",
    "    \n",
    "    # Get tokenized sentence and entities\n",
    "    tokens = [token.text for token in doc]\n",
    "    entities = [(ent.text, ent.start, ent.end, ent.label_) for ent in doc.ents]\n",
    "    \n",
    "    # Convert tokens to IDs\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentence))\n",
    "    \n",
    "    # Generate position embeddings relative to each entity\n",
    "    pos_embedding = [\n",
    "        [i - ent_start for _, ent_start, _, _ in entities]\n",
    "        for i in range(len(tokens))\n",
    "    ]\n",
    "    \n",
    "    # Extract unique entity labels\n",
    "    entity_labels = {ent.label_ for ent in doc.ents}\n",
    "    \n",
    "    # Return results\n",
    "    return {\n",
    "        \"tokens\": tokens,\n",
    "        \"token_ids\": token_ids,\n",
    "        \"entities\": entities,\n",
    "        \"position_embeddings\": pos_embedding,\n",
    "        \"entity_labels\": list(entity_labels)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f86fcd30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': ['Apple',\n",
       "  'Inc.',\n",
       "  'is',\n",
       "  'a',\n",
       "  'technology',\n",
       "  'company',\n",
       "  'headquartered',\n",
       "  'in',\n",
       "  'Cupertino',\n",
       "  ',',\n",
       "  'California',\n",
       "  '.',\n",
       "  '\\n',\n",
       "  'Founded',\n",
       "  'by',\n",
       "  'Steve',\n",
       "  'Jobs',\n",
       "  ',',\n",
       "  'Steve',\n",
       "  'Wozniak',\n",
       "  ',',\n",
       "  'and',\n",
       "  'Ronald',\n",
       "  'Wayne',\n",
       "  'in',\n",
       "  '1976',\n",
       "  ',',\n",
       "  '\\n',\n",
       "  'it',\n",
       "  'has',\n",
       "  'become',\n",
       "  'one',\n",
       "  'of',\n",
       "  'the',\n",
       "  'leading',\n",
       "  'innovators',\n",
       "  'in',\n",
       "  'consumer',\n",
       "  'electronics',\n",
       "  '.',\n",
       "  '\\n',\n",
       "  'Apple',\n",
       "  'is',\n",
       "  'known',\n",
       "  'for',\n",
       "  'its',\n",
       "  'iPhone',\n",
       "  ',',\n",
       "  'iPad',\n",
       "  ',',\n",
       "  'and',\n",
       "  'Mac',\n",
       "  'products',\n",
       "  '.',\n",
       "  '\\n',\n",
       "  'The',\n",
       "  'company',\n",
       "  'is',\n",
       "  'also',\n",
       "  'involved',\n",
       "  'in',\n",
       "  'the',\n",
       "  'development',\n",
       "  'of',\n",
       "  'software',\n",
       "  'and',\n",
       "  'services',\n",
       "  'such',\n",
       "  'as',\n",
       "  'iCloud',\n",
       "  'and',\n",
       "  'Apple',\n",
       "  'Music',\n",
       "  '.',\n",
       "  '\\n',\n",
       "  'In',\n",
       "  'recent',\n",
       "  'years',\n",
       "  ',',\n",
       "  'Apple',\n",
       "  'has',\n",
       "  'faced',\n",
       "  'scrutiny',\n",
       "  'over',\n",
       "  'issues',\n",
       "  'related',\n",
       "  'to',\n",
       "  'data',\n",
       "  'privacy',\n",
       "  'and',\n",
       "  'labor',\n",
       "  'practices',\n",
       "  'in',\n",
       "  'its',\n",
       "  'supply',\n",
       "  'chain',\n",
       "  '.',\n",
       "  '\\n'],\n",
       " 'token_ids': [6207,\n",
       "  4297,\n",
       "  1012,\n",
       "  2003,\n",
       "  1037,\n",
       "  2974,\n",
       "  2194,\n",
       "  9403,\n",
       "  1999,\n",
       "  2452,\n",
       "  8743,\n",
       "  5740,\n",
       "  1010,\n",
       "  2662,\n",
       "  1012,\n",
       "  2631,\n",
       "  2011,\n",
       "  3889,\n",
       "  5841,\n",
       "  1010,\n",
       "  3889,\n",
       "  24185,\n",
       "  2480,\n",
       "  6200,\n",
       "  2243,\n",
       "  1010,\n",
       "  1998,\n",
       "  8923,\n",
       "  6159,\n",
       "  1999,\n",
       "  3299,\n",
       "  1010,\n",
       "  2009,\n",
       "  2038,\n",
       "  2468,\n",
       "  2028,\n",
       "  1997,\n",
       "  1996,\n",
       "  2877,\n",
       "  7601,\n",
       "  7103,\n",
       "  6591,\n",
       "  1999,\n",
       "  7325,\n",
       "  8139,\n",
       "  1012,\n",
       "  6207,\n",
       "  2003,\n",
       "  2124,\n",
       "  2005,\n",
       "  2049,\n",
       "  18059,\n",
       "  1010,\n",
       "  25249,\n",
       "  1010,\n",
       "  1998,\n",
       "  6097,\n",
       "  3688,\n",
       "  1012,\n",
       "  1996,\n",
       "  2194,\n",
       "  2003,\n",
       "  2036,\n",
       "  2920,\n",
       "  1999,\n",
       "  1996,\n",
       "  2458,\n",
       "  1997,\n",
       "  4007,\n",
       "  1998,\n",
       "  2578,\n",
       "  2107,\n",
       "  2004,\n",
       "  24582,\n",
       "  23743,\n",
       "  2094,\n",
       "  1998,\n",
       "  6207,\n",
       "  2189,\n",
       "  1012,\n",
       "  1999,\n",
       "  3522,\n",
       "  2086,\n",
       "  1010,\n",
       "  6207,\n",
       "  2038,\n",
       "  4320,\n",
       "  17423,\n",
       "  2058,\n",
       "  3314,\n",
       "  3141,\n",
       "  2000,\n",
       "  2951,\n",
       "  9394,\n",
       "  1998,\n",
       "  4450,\n",
       "  6078,\n",
       "  1999,\n",
       "  2049,\n",
       "  4425,\n",
       "  4677,\n",
       "  1012],\n",
       " 'entities': [('Apple Inc.', 0, 2, 'ORG'),\n",
       "  ('Cupertino', 8, 9, 'GPE'),\n",
       "  ('California', 10, 11, 'GPE'),\n",
       "  ('Steve Jobs', 15, 17, 'PERSON'),\n",
       "  ('Steve Wozniak', 18, 20, 'PERSON'),\n",
       "  ('Ronald Wayne', 22, 24, 'PERSON'),\n",
       "  ('1976', 25, 26, 'DATE'),\n",
       "  ('Apple', 41, 42, 'ORG'),\n",
       "  ('iPhone', 46, 47, 'ORG'),\n",
       "  ('iPad', 48, 49, 'ORG'),\n",
       "  ('Mac', 51, 52, 'PERSON'),\n",
       "  ('iCloud', 69, 70, 'ORG'),\n",
       "  ('Apple Music', 71, 73, 'ORG'),\n",
       "  ('recent years', 76, 78, 'DATE'),\n",
       "  ('Apple', 79, 80, 'ORG')],\n",
       " 'position_embeddings': [[0,\n",
       "   -8,\n",
       "   -10,\n",
       "   -15,\n",
       "   -18,\n",
       "   -22,\n",
       "   -25,\n",
       "   -41,\n",
       "   -46,\n",
       "   -48,\n",
       "   -51,\n",
       "   -69,\n",
       "   -71,\n",
       "   -76,\n",
       "   -79],\n",
       "  [1, -7, -9, -14, -17, -21, -24, -40, -45, -47, -50, -68, -70, -75, -78],\n",
       "  [2, -6, -8, -13, -16, -20, -23, -39, -44, -46, -49, -67, -69, -74, -77],\n",
       "  [3, -5, -7, -12, -15, -19, -22, -38, -43, -45, -48, -66, -68, -73, -76],\n",
       "  [4, -4, -6, -11, -14, -18, -21, -37, -42, -44, -47, -65, -67, -72, -75],\n",
       "  [5, -3, -5, -10, -13, -17, -20, -36, -41, -43, -46, -64, -66, -71, -74],\n",
       "  [6, -2, -4, -9, -12, -16, -19, -35, -40, -42, -45, -63, -65, -70, -73],\n",
       "  [7, -1, -3, -8, -11, -15, -18, -34, -39, -41, -44, -62, -64, -69, -72],\n",
       "  [8, 0, -2, -7, -10, -14, -17, -33, -38, -40, -43, -61, -63, -68, -71],\n",
       "  [9, 1, -1, -6, -9, -13, -16, -32, -37, -39, -42, -60, -62, -67, -70],\n",
       "  [10, 2, 0, -5, -8, -12, -15, -31, -36, -38, -41, -59, -61, -66, -69],\n",
       "  [11, 3, 1, -4, -7, -11, -14, -30, -35, -37, -40, -58, -60, -65, -68],\n",
       "  [12, 4, 2, -3, -6, -10, -13, -29, -34, -36, -39, -57, -59, -64, -67],\n",
       "  [13, 5, 3, -2, -5, -9, -12, -28, -33, -35, -38, -56, -58, -63, -66],\n",
       "  [14, 6, 4, -1, -4, -8, -11, -27, -32, -34, -37, -55, -57, -62, -65],\n",
       "  [15, 7, 5, 0, -3, -7, -10, -26, -31, -33, -36, -54, -56, -61, -64],\n",
       "  [16, 8, 6, 1, -2, -6, -9, -25, -30, -32, -35, -53, -55, -60, -63],\n",
       "  [17, 9, 7, 2, -1, -5, -8, -24, -29, -31, -34, -52, -54, -59, -62],\n",
       "  [18, 10, 8, 3, 0, -4, -7, -23, -28, -30, -33, -51, -53, -58, -61],\n",
       "  [19, 11, 9, 4, 1, -3, -6, -22, -27, -29, -32, -50, -52, -57, -60],\n",
       "  [20, 12, 10, 5, 2, -2, -5, -21, -26, -28, -31, -49, -51, -56, -59],\n",
       "  [21, 13, 11, 6, 3, -1, -4, -20, -25, -27, -30, -48, -50, -55, -58],\n",
       "  [22, 14, 12, 7, 4, 0, -3, -19, -24, -26, -29, -47, -49, -54, -57],\n",
       "  [23, 15, 13, 8, 5, 1, -2, -18, -23, -25, -28, -46, -48, -53, -56],\n",
       "  [24, 16, 14, 9, 6, 2, -1, -17, -22, -24, -27, -45, -47, -52, -55],\n",
       "  [25, 17, 15, 10, 7, 3, 0, -16, -21, -23, -26, -44, -46, -51, -54],\n",
       "  [26, 18, 16, 11, 8, 4, 1, -15, -20, -22, -25, -43, -45, -50, -53],\n",
       "  [27, 19, 17, 12, 9, 5, 2, -14, -19, -21, -24, -42, -44, -49, -52],\n",
       "  [28, 20, 18, 13, 10, 6, 3, -13, -18, -20, -23, -41, -43, -48, -51],\n",
       "  [29, 21, 19, 14, 11, 7, 4, -12, -17, -19, -22, -40, -42, -47, -50],\n",
       "  [30, 22, 20, 15, 12, 8, 5, -11, -16, -18, -21, -39, -41, -46, -49],\n",
       "  [31, 23, 21, 16, 13, 9, 6, -10, -15, -17, -20, -38, -40, -45, -48],\n",
       "  [32, 24, 22, 17, 14, 10, 7, -9, -14, -16, -19, -37, -39, -44, -47],\n",
       "  [33, 25, 23, 18, 15, 11, 8, -8, -13, -15, -18, -36, -38, -43, -46],\n",
       "  [34, 26, 24, 19, 16, 12, 9, -7, -12, -14, -17, -35, -37, -42, -45],\n",
       "  [35, 27, 25, 20, 17, 13, 10, -6, -11, -13, -16, -34, -36, -41, -44],\n",
       "  [36, 28, 26, 21, 18, 14, 11, -5, -10, -12, -15, -33, -35, -40, -43],\n",
       "  [37, 29, 27, 22, 19, 15, 12, -4, -9, -11, -14, -32, -34, -39, -42],\n",
       "  [38, 30, 28, 23, 20, 16, 13, -3, -8, -10, -13, -31, -33, -38, -41],\n",
       "  [39, 31, 29, 24, 21, 17, 14, -2, -7, -9, -12, -30, -32, -37, -40],\n",
       "  [40, 32, 30, 25, 22, 18, 15, -1, -6, -8, -11, -29, -31, -36, -39],\n",
       "  [41, 33, 31, 26, 23, 19, 16, 0, -5, -7, -10, -28, -30, -35, -38],\n",
       "  [42, 34, 32, 27, 24, 20, 17, 1, -4, -6, -9, -27, -29, -34, -37],\n",
       "  [43, 35, 33, 28, 25, 21, 18, 2, -3, -5, -8, -26, -28, -33, -36],\n",
       "  [44, 36, 34, 29, 26, 22, 19, 3, -2, -4, -7, -25, -27, -32, -35],\n",
       "  [45, 37, 35, 30, 27, 23, 20, 4, -1, -3, -6, -24, -26, -31, -34],\n",
       "  [46, 38, 36, 31, 28, 24, 21, 5, 0, -2, -5, -23, -25, -30, -33],\n",
       "  [47, 39, 37, 32, 29, 25, 22, 6, 1, -1, -4, -22, -24, -29, -32],\n",
       "  [48, 40, 38, 33, 30, 26, 23, 7, 2, 0, -3, -21, -23, -28, -31],\n",
       "  [49, 41, 39, 34, 31, 27, 24, 8, 3, 1, -2, -20, -22, -27, -30],\n",
       "  [50, 42, 40, 35, 32, 28, 25, 9, 4, 2, -1, -19, -21, -26, -29],\n",
       "  [51, 43, 41, 36, 33, 29, 26, 10, 5, 3, 0, -18, -20, -25, -28],\n",
       "  [52, 44, 42, 37, 34, 30, 27, 11, 6, 4, 1, -17, -19, -24, -27],\n",
       "  [53, 45, 43, 38, 35, 31, 28, 12, 7, 5, 2, -16, -18, -23, -26],\n",
       "  [54, 46, 44, 39, 36, 32, 29, 13, 8, 6, 3, -15, -17, -22, -25],\n",
       "  [55, 47, 45, 40, 37, 33, 30, 14, 9, 7, 4, -14, -16, -21, -24],\n",
       "  [56, 48, 46, 41, 38, 34, 31, 15, 10, 8, 5, -13, -15, -20, -23],\n",
       "  [57, 49, 47, 42, 39, 35, 32, 16, 11, 9, 6, -12, -14, -19, -22],\n",
       "  [58, 50, 48, 43, 40, 36, 33, 17, 12, 10, 7, -11, -13, -18, -21],\n",
       "  [59, 51, 49, 44, 41, 37, 34, 18, 13, 11, 8, -10, -12, -17, -20],\n",
       "  [60, 52, 50, 45, 42, 38, 35, 19, 14, 12, 9, -9, -11, -16, -19],\n",
       "  [61, 53, 51, 46, 43, 39, 36, 20, 15, 13, 10, -8, -10, -15, -18],\n",
       "  [62, 54, 52, 47, 44, 40, 37, 21, 16, 14, 11, -7, -9, -14, -17],\n",
       "  [63, 55, 53, 48, 45, 41, 38, 22, 17, 15, 12, -6, -8, -13, -16],\n",
       "  [64, 56, 54, 49, 46, 42, 39, 23, 18, 16, 13, -5, -7, -12, -15],\n",
       "  [65, 57, 55, 50, 47, 43, 40, 24, 19, 17, 14, -4, -6, -11, -14],\n",
       "  [66, 58, 56, 51, 48, 44, 41, 25, 20, 18, 15, -3, -5, -10, -13],\n",
       "  [67, 59, 57, 52, 49, 45, 42, 26, 21, 19, 16, -2, -4, -9, -12],\n",
       "  [68, 60, 58, 53, 50, 46, 43, 27, 22, 20, 17, -1, -3, -8, -11],\n",
       "  [69, 61, 59, 54, 51, 47, 44, 28, 23, 21, 18, 0, -2, -7, -10],\n",
       "  [70, 62, 60, 55, 52, 48, 45, 29, 24, 22, 19, 1, -1, -6, -9],\n",
       "  [71, 63, 61, 56, 53, 49, 46, 30, 25, 23, 20, 2, 0, -5, -8],\n",
       "  [72, 64, 62, 57, 54, 50, 47, 31, 26, 24, 21, 3, 1, -4, -7],\n",
       "  [73, 65, 63, 58, 55, 51, 48, 32, 27, 25, 22, 4, 2, -3, -6],\n",
       "  [74, 66, 64, 59, 56, 52, 49, 33, 28, 26, 23, 5, 3, -2, -5],\n",
       "  [75, 67, 65, 60, 57, 53, 50, 34, 29, 27, 24, 6, 4, -1, -4],\n",
       "  [76, 68, 66, 61, 58, 54, 51, 35, 30, 28, 25, 7, 5, 0, -3],\n",
       "  [77, 69, 67, 62, 59, 55, 52, 36, 31, 29, 26, 8, 6, 1, -2],\n",
       "  [78, 70, 68, 63, 60, 56, 53, 37, 32, 30, 27, 9, 7, 2, -1],\n",
       "  [79, 71, 69, 64, 61, 57, 54, 38, 33, 31, 28, 10, 8, 3, 0],\n",
       "  [80, 72, 70, 65, 62, 58, 55, 39, 34, 32, 29, 11, 9, 4, 1],\n",
       "  [81, 73, 71, 66, 63, 59, 56, 40, 35, 33, 30, 12, 10, 5, 2],\n",
       "  [82, 74, 72, 67, 64, 60, 57, 41, 36, 34, 31, 13, 11, 6, 3],\n",
       "  [83, 75, 73, 68, 65, 61, 58, 42, 37, 35, 32, 14, 12, 7, 4],\n",
       "  [84, 76, 74, 69, 66, 62, 59, 43, 38, 36, 33, 15, 13, 8, 5],\n",
       "  [85, 77, 75, 70, 67, 63, 60, 44, 39, 37, 34, 16, 14, 9, 6],\n",
       "  [86, 78, 76, 71, 68, 64, 61, 45, 40, 38, 35, 17, 15, 10, 7],\n",
       "  [87, 79, 77, 72, 69, 65, 62, 46, 41, 39, 36, 18, 16, 11, 8],\n",
       "  [88, 80, 78, 73, 70, 66, 63, 47, 42, 40, 37, 19, 17, 12, 9],\n",
       "  [89, 81, 79, 74, 71, 67, 64, 48, 43, 41, 38, 20, 18, 13, 10],\n",
       "  [90, 82, 80, 75, 72, 68, 65, 49, 44, 42, 39, 21, 19, 14, 11],\n",
       "  [91, 83, 81, 76, 73, 69, 66, 50, 45, 43, 40, 22, 20, 15, 12],\n",
       "  [92, 84, 82, 77, 74, 70, 67, 51, 46, 44, 41, 23, 21, 16, 13],\n",
       "  [93, 85, 83, 78, 75, 71, 68, 52, 47, 45, 42, 24, 22, 17, 14],\n",
       "  [94, 86, 84, 79, 76, 72, 69, 53, 48, 46, 43, 25, 23, 18, 15],\n",
       "  [95, 87, 85, 80, 77, 73, 70, 54, 49, 47, 44, 26, 24, 19, 16],\n",
       "  [96, 88, 86, 81, 78, 74, 71, 55, 50, 48, 45, 27, 25, 20, 17],\n",
       "  [97, 89, 87, 82, 79, 75, 72, 56, 51, 49, 46, 28, 26, 21, 18]],\n",
       " 'entity_labels': ['DATE', 'PERSON', 'GPE', 'ORG']}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's see an example\n",
    "sentence = \"\"\"Apple Inc. is a technology company headquartered in Cupertino, California. \n",
    "Founded by Steve Jobs, Steve Wozniak, and Ronald Wayne in 1976, \n",
    "it has become one of the leading innovators in consumer electronics. \n",
    "Apple is known for its iPhone, iPad, and Mac products. \n",
    "The company is also involved in the development of software and services such as iCloud and Apple Music. \n",
    "In recent years, Apple has faced scrutiny over issues related to data privacy and labor practices in its supply chain.\n",
    "\"\"\"\n",
    "preprocess_sentence(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e5a66b",
   "metadata": {},
   "source": [
    "### Split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "40563e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = train_test_split(data['text'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7af96bc",
   "metadata": {},
   "source": [
    "### Transform the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b1e15804",
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0d1a291a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_vect = vect.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "35dbf299",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_vect = vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0c3352f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "X_train_red = pca.fit_transform(X_train_vect.toarray())\n",
    "X_test_red = pca.transform(X_test_vect.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "af7a8e17-4fc9-40fd-a7b4-50347d95dc3c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Tokens: ['This', 'paper', 'presents', 'a', 'novel', 'representation', 'for', 'three-dimensional', 'objects', 'in', 'terms', 'of', 'affine-invariant', 'image', 'patches', 'and', 'their', 'spatial', 'relationships', '.']\n",
      "Sample Entities: []\n",
      "Sample Dependencies: [('ROOT', 'presents', 'root'), ('paper', 'This', 'det'), ('presents', 'paper', 'nsubj'), ('representation', 'a', 'det'), ('representation', 'novel', 'amod'), ('presents', 'representation', 'dobj'), ('representation', 'for', 'prep'), ('objects', 'three-dimensional', 'amod'), ('for', 'objects', 'pobj'), ('representation', 'in', 'prep'), ('in', 'terms', 'pobj'), ('terms', 'of', 'prep'), ('patches', 'affine-invariant', 'amod'), ('patches', 'image', 'nn'), ('of', 'patches', 'pobj'), ('patches', 'and', 'cc'), ('relationships', 'their', 'poss'), ('relationships', 'spatial', 'amod'), ('patches', 'relationships', 'conj'), ('presents', '.', 'punct')]\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# Load the XML file\n",
    "tree = ET.parse('C:/Personal/ML/Relation Extraction Project/data/raw_data/CVPR_2003_30_abs.txt.xml')\n",
    "root = tree.getroot()\n",
    "\n",
    "# Function to extract tokens, NER, and dependencies\n",
    "def extract_data(root):\n",
    "    tokens = []\n",
    "    entities = []\n",
    "    dependencies = []\n",
    "    \n",
    "    for sentence in root.findall(\".//sentence\"):\n",
    "        sent_tokens = []\n",
    "        sent_entities = []\n",
    "        sent_dependencies = []\n",
    "        \n",
    "        # Extract tokens and entities\n",
    "        for token in sentence.findall(\".//token\"):\n",
    "            word = token.find(\"word\").text\n",
    "            ner = token.find(\"NER\").text\n",
    "            sent_tokens.append(word)\n",
    "            if ner != 'O':  # 'O' indicates no entity\n",
    "                sent_entities.append((word, ner))\n",
    "        \n",
    "        # Extract dependencies\n",
    "        for dep in sentence.findall(\".//dependencies[@type='basic-dependencies']//dep\"):\n",
    "            governor = dep.find(\"governor\").text\n",
    "            dependent = dep.find(\"dependent\").text\n",
    "            relation = dep.get(\"type\")\n",
    "            sent_dependencies.append((governor, dependent, relation))\n",
    "        \n",
    "        tokens.append(sent_tokens)\n",
    "        entities.append(sent_entities)\n",
    "        dependencies.append(sent_dependencies)\n",
    "    \n",
    "    return tokens, entities, dependencies\n",
    "\n",
    "# Extract data from the XML\n",
    "tokens, entities, dependencies = extract_data(root)\n",
    "\n",
    "# Print a sample of extracted data\n",
    "print(\"Sample Tokens:\", tokens[0])\n",
    "print(\"Sample Entities:\", entities[0])\n",
    "print(\"Sample Dependencies:\", dependencies[0])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
